{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab1676d",
   "metadata": {
    "papermill": {
     "duration": 0.010054,
     "end_time": "2023-10-09T13:43:10.590987",
     "exception": false,
     "start_time": "2023-10-09T13:43:10.580933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LLM Science Exam Optimise Ensemble Weights \n",
    "\n",
    "In this competition, when looking for the high-scoring notebooks, those that are ensembles with multiple models stand out. In fact, it is known empirically that ensembles are very powerful in NLP competition.\n",
    "\n",
    "[The voting ensemble was introduced](https://www.kaggle.com/code/radek1/an-introduction-to-voting-ensemble) by [radek1](https://www.kaggle.com/radek1) and many notes have been published on this basis.\n",
    "\n",
    "On the other hand, ensembles with predicted probabilities appear to be less used.\n",
    "\n",
    "This notebook introduces ensembles using probabilities and shows how to optimise model weights with **scipy.optimize**.\n",
    "\n",
    "Normally, OOF(out of fold) predictions are used to optimise model weights, But The training data used looks mixed and most of the weight is for single models. Therefore, I'll use an evaluation dataset that appears not to have been used for training. the dataset named [MMLU-Dataset](https://www.kaggle.com/datasets/peiyuanliu2001/mmlu-dataset) shared by [Peiyuan Liu](https://www.kaggle.com/peiyuanliu2001). [See his discussion for details.](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/433168) Please note that this dataset contains more than just STEM questions, so it may not be suitable as an evaluation dataset.\n",
    "\n",
    "edit: Somehow unable to submitted due to the MMLU dataset, so I've created a separate dataset.\n",
    "\n",
    "edit: [Chris Deotte](https://www.kaggle.com/cdeotte) once again published an [amazing dataset](https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2) and notebooks. his [training code is here](https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1) and [inference code is here](https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-2). This version also uses his trained weights.\n",
    "\n",
    "### References, see also them\n",
    "\n",
    "Weight optimization related \n",
    "\n",
    "* [Optimise Blending Weights with Bonus :0](https://www.kaggle.com/code/gogo827jz/optimise-blending-weights-with-bonus-0/notebook) by [Yirun Zhang](https://www.kaggle.com/gogo827jz)\n",
    "\n",
    "OpenBook and its tuning related(Too many, so just partial only)\n",
    "\n",
    "* [OpenBook DeBERTaV3-Large Baseline (Single Model)](https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model) by [Anil Ozturk](https://www.kaggle.com/nlztrk)\n",
    "\n",
    "* [[0.807] Sharing my trained-with-context model](https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model/notebook) by [MGÃ¶ksu](https://www.kaggle.com/mgoksu)\n",
    "\n",
    "Trainning and inferring OpenBook Dataset with context\n",
    "\n",
    "* [How To Train Open Book Model - Part 1](https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1) by [Chris Deotte](https://www.kaggle.com/cdeotte)\n",
    "\n",
    "* [How To Train Open Book Model - Part 2](https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-2) by [Chris Deotte](https://www.kaggle.com/cdeotte)\n",
    "\n",
    "Voting ensemble (Too many, so just the original)\n",
    "\n",
    "* [The voting ensemble was introduced](https://www.kaggle.com/code/radek1/an-introduction-to-voting-ensemble) by [radek1](https://www.kaggle.com/radek1)\n",
    "\n",
    "### My other Notebooks\n",
    "\n",
    "In this competition\n",
    "\n",
    "* [Incorporate MAP@k metrics into HF Trainer](https://www.kaggle.com/code/itsuki9180/incorporate-map-k-metrics-into-hf-trainer)\n",
    "\n",
    "* [Introducing Adversarial Weight Perturbation (AWP)](https://www.kaggle.com/code/itsuki9180/introducing-adversarial-weight-perturbation-awp)\n",
    "\n",
    "* [Adversarial Weight Perturbation (AWP) Inference](https://www.kaggle.com/code/itsuki9180/adversarial-weight-perturbation-awp-inference)\n",
    "\n",
    "* [Using DeepSpeed with HFðŸ¤— Trainer](https://www.kaggle.com/code/itsuki9180/using-deepspeed-with-hf-trainer)\n",
    "\n",
    "Weight optimization related (almost same as Yirun Zhangs')\n",
    "\n",
    "* [G2Net_oof_weight_optimizer](https://www.kaggle.com/code/itsuki9180/g2net-oof-weight-optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72331711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:43:10.612302Z",
     "iopub.status.busy": "2023-10-09T13:43:10.611962Z",
     "iopub.status.idle": "2023-10-09T13:45:30.436573Z",
     "shell.execute_reply": "2023-10-09T13:45:30.435421Z"
    },
    "papermill": {
     "duration": 139.83752,
     "end_time": "2023-10-09T13:45:30.438785",
     "exception": false,
     "start_time": "2023-10-09T13:43:10.601265",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: faiss-gpu\r\n",
      "Successfully installed faiss-gpu-1.7.2\r\n",
      "Processing ./sentence-transformers\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.33.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.1)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.0.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.15.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.23.5)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.11.2)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.2.4)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.99)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.12.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.9.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.6.3)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.6.3)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.3.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.16.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.1.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\r\n",
      "Building wheels for collected packages: sentence-transformers\r\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=126125 sha256=af71f334818656184faba28410e915a145c26916afd175d397b364605e34180b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6c/ea/76/d9a930b223b1d3d5d6aff69458725316b0fe205b854faf1812\r\n",
      "Successfully built sentence-transformers\r\n",
      "Installing collected packages: sentence-transformers\r\n",
      "Successfully installed sentence-transformers-2.2.2\r\n",
      "Processing /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\r\n",
      "Installing collected packages: blingfire\r\n",
      "Successfully installed blingfire-0.1.8\r\n",
      "Processing /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.33.0\r\n",
      "    Uninstalling transformers-4.33.0:\r\n",
      "      Successfully uninstalled transformers-4.33.0\r\n",
      "Successfully installed transformers-4.31.0\r\n",
      "Processing /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: peft\r\n",
      "Successfully installed peft-0.4.0\r\n",
      "Processing /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\r\n",
      "Installing collected packages: datasets\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 2.1.0\r\n",
      "    Uninstalling datasets-2.1.0:\r\n",
      "      Successfully uninstalled datasets-2.1.0\r\n",
      "Successfully installed datasets-2.14.3\r\n",
      "Processing /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\r\n",
      "Installing collected packages: trl\r\n",
      "Successfully installed trl-0.5.0\r\n"
     ]
    }
   ],
   "source": [
    "# installing offline dependencies\n",
    "!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n",
    "\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30542fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:45:30.462976Z",
     "iopub.status.busy": "2023-10-09T13:45:30.462645Z",
     "iopub.status.idle": "2023-10-09T13:45:45.004336Z",
     "shell.execute_reply": "2023-10-09T13:45:45.003421Z"
    },
    "papermill": {
     "duration": 14.556614,
     "end_time": "2023-10-09T13:45:45.006496",
     "exception": false,
     "start_time": "2023-10-09T13:45:30.449882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d86d315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:45:45.031333Z",
     "iopub.status.busy": "2023-10-09T13:45:45.030357Z",
     "iopub.status.idle": "2023-10-09T13:45:45.075206Z",
     "shell.execute_reply": "2023-10-09T13:45:45.074300Z"
    },
    "papermill": {
     "duration": 0.059032,
     "end_time": "2023-10-09T13:45:45.077208",
     "exception": false,
     "start_time": "2023-10-09T13:45:45.018176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n",
    "DEVICE = 0\n",
    "MAX_LENGTH = 384\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\n",
    "\n",
    "DEBUG = False\n",
    "# DEBUG = False if len(trn)!=200 else True # If you want to save GPU Quota, check off this comment-out. But cannot get accurate weight on saving notebook\n",
    "FILTER_LEN = 1 if DEBUG else 10\n",
    "IND_SEARCH = 1 if DEBUG else 7\n",
    "NUM_SENTENCES_INCLUDE = 1 if DEBUG else 22\n",
    "CONTEXT_LEN = 1000 if DEBUG else 2305\n",
    "IS_TEST_SET = len(trn) != 200\n",
    "\n",
    "WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n",
    "wiki_files = os.listdir(WIKI_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54500a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:45:45.100872Z",
     "iopub.status.busy": "2023-10-09T13:45:45.100591Z",
     "iopub.status.idle": "2023-10-09T13:45:45.109952Z",
     "shell.execute_reply": "2023-10-09T13:45:45.108999Z"
    },
    "papermill": {
     "duration": 0.023417,
     "end_time": "2023-10-09T13:45:45.111652",
     "exception": false,
     "start_time": "2023-10-09T13:45:45.088235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_documents(documents: Iterable[str],\n",
    "                      document_ids: Iterable,\n",
    "                      split_sentences: bool = True,\n",
    "                      filter_len: int = FILTER_LEN,\n",
    "                      disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \n",
    "    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n",
    "\n",
    "    if split_sentences:\n",
    "        df = sentencize(df.text.values, \n",
    "                        df.document_id.values,\n",
    "                        df.offset.values, \n",
    "                        filter_len, \n",
    "                        disable_progress_bar)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sectionize_documents(documents: Iterable[str],\n",
    "                         document_ids: Iterable,\n",
    "                         disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "\n",
    "    processed_documents = []\n",
    "    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row['document_id'] = document_id\n",
    "        row['text'] = text\n",
    "        row['offset'] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def sentencize(documents: Iterable[str],\n",
    "               document_ids: Iterable,\n",
    "               offsets: Iterable[tuple[int, int]],\n",
    "               filter_len: int = FILTER_LEN,\n",
    "               disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "\n",
    "    document_sentences = []\n",
    "    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n",
    "        try:\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            for o in sentence_offsets:\n",
    "                if o[1]-o[0] > filter_len:\n",
    "                    sentence = document[o[0]:o[1]]\n",
    "                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n",
    "                    row = {}\n",
    "                    row['document_id'] = document_id\n",
    "                    row['text'] = sentence\n",
    "                    row['offset'] = abs_offsets\n",
    "                    document_sentences.append(row)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(document_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb351bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:45:45.134343Z",
     "iopub.status.busy": "2023-10-09T13:45:45.134018Z",
     "iopub.status.idle": "2023-10-09T13:45:45.154191Z",
     "shell.execute_reply": "2023-10-09T13:45:45.153374Z"
    },
    "papermill": {
     "duration": 0.033595,
     "end_time": "2023-10-09T13:45:45.155910",
     "exception": false,
     "start_time": "2023-10-09T13:45:45.122315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>MOND is a theory that reduces the observed mis...</td>\n",
       "      <td>MOND is a theory that increases the discrepanc...</td>\n",
       "      <td>MOND is a theory that explains the missing bar...</td>\n",
       "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
       "      <td>MOND is a theory that eliminates the observed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which of the following is an accurate definiti...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>The triskeles symbol is a representation of a ...</td>\n",
       "      <td>The triskeles symbol represents three interloc...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the significance of regularization in ...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Which of the following statements accurately d...   \n",
       "1  Which of the following is an accurate definiti...   \n",
       "2  Which of the following statements accurately d...   \n",
       "3  What is the significance of regularization in ...   \n",
       "4  Which of the following statements accurately d...   \n",
       "\n",
       "                                                   A  \\\n",
       "0  MOND is a theory that reduces the observed mis...   \n",
       "1  Dynamic scaling refers to the evolution of sel...   \n",
       "2  The triskeles symbol was reconstructed as a fe...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   B  \\\n",
       "0  MOND is a theory that increases the discrepanc...   \n",
       "1  Dynamic scaling refers to the non-evolution of...   \n",
       "2  The triskeles symbol is a representation of th...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   C  \\\n",
       "0  MOND is a theory that explains the missing bar...   \n",
       "1  Dynamic scaling refers to the evolution of sel...   \n",
       "2  The triskeles symbol is a representation of a ...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   D  \\\n",
       "0  MOND is a theory that reduces the discrepancy ...   \n",
       "1  Dynamic scaling refers to the non-evolution of...   \n",
       "2  The triskeles symbol represents three interloc...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   E  \n",
       "0  MOND is a theory that eliminates the observed ...  \n",
       "1  Dynamic scaling refers to the evolution of sel...  \n",
       "2  The triskeles symbol is a representation of th...  \n",
       "3  Regularizing the mass-energy of an electron wi...  \n",
       "4  The angular spacing of features in the diffrac...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1c29f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:45:45.179960Z",
     "iopub.status.busy": "2023-10-09T13:45:45.179640Z",
     "iopub.status.idle": "2023-10-09T13:45:45.185281Z",
     "shell.execute_reply": "2023-10-09T13:45:45.184403Z"
    },
    "papermill": {
     "duration": 0.019971,
     "end_time": "2023-10-09T13:45:45.187030",
     "exception": false,
     "start_time": "2023-10-09T13:45:45.167059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    model = SentenceTransformer(SIM_MODEL, device='cuda')\n",
    "    model.max_seq_length = MAX_LENGTH\n",
    "    model = model.half()\n",
    "\n",
    "    sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n",
    "\n",
    "    prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    _ = gc.collect()\n",
    "\n",
    "    ## Get the top IND_SEARCH pages that are likely to contain the topic of interest\n",
    "    search_score, search_index = sentence_index.search(prompt_embeddings, IND_SEARCH)\n",
    "\n",
    "    ## Save memory - delete sentence_index since it is no longer necessary\n",
    "    del sentence_index\n",
    "    del prompt_embeddings\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6124b33",
   "metadata": {
    "papermill": {
     "duration": 0.010805,
     "end_time": "2023-10-09T13:45:45.208710",
     "exception": false,
     "start_time": "2023-10-09T13:45:45.197905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [86.2] with only 270K articles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c115cb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:45:45.232292Z",
     "iopub.status.busy": "2023-10-09T13:45:45.231636Z",
     "iopub.status.idle": "2023-10-09T13:46:20.454888Z",
     "shell.execute_reply": "2023-10-09T13:46:20.453554Z"
    },
    "papermill": {
     "duration": 35.237839,
     "end_time": "2023-10-09T13:46:20.457447",
     "exception": false,
     "start_time": "2023-10-09T13:45:45.219608",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./datasets-2.14.4-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (1.23.5)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (11.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.0.2)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (4.66.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.3.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2023.9.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.8.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.16.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (6.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (3.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (4.6.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.14.4) (3.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (2023.7.22)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.4) (1.16.0)\r\n",
      "Installing collected packages: datasets\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 2.14.3\r\n",
      "    Uninstalling datasets-2.14.3:\r\n",
      "      Successfully uninstalled datasets-2.14.3\r\n",
      "Successfully installed datasets-2.14.4\r\n",
      "cp: cannot stat '/kaggle/input/backup-806/util_openbook.py': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n",
    "!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n",
    "!cp /kaggle/input/backup-806/util_openbook.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccea315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:20.484647Z",
     "iopub.status.busy": "2023-10-09T13:46:20.483885Z",
     "iopub.status.idle": "2023-10-09T13:46:20.521202Z",
     "shell.execute_reply": "2023-10-09T13:46:20.520280Z"
    },
    "papermill": {
     "duration": 0.053434,
     "end_time": "2023-10-09T13:46:20.523790",
     "exception": false,
     "start_time": "2023-10-09T13:46:20.470356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def process_documents(documents: Iterable[str],\n",
    "                      document_ids: Iterable,\n",
    "                      split_sentences: bool = True,\n",
    "                      filter_len: int = 3,\n",
    "                      disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main helper function to process documents from the EMR.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param document_type: String denoting the document type to be processed\n",
    "    :param document_sections: List of sections for a given document type to process\n",
    "    :param split_sentences: Flag to determine whether to further split sections into sentences\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n",
    "\n",
    "    if split_sentences:\n",
    "        df = sentencize(df.text.values,\n",
    "                        df.document_id.values,\n",
    "                        df.offset.values,\n",
    "                        filter_len,\n",
    "                        disable_progress_bar)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sectionize_documents(documents: Iterable[str],\n",
    "                         document_ids: Iterable,\n",
    "                         disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the\n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row['document_id'] = document_id\n",
    "        row['text'] = text\n",
    "        row['offset'] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def sentencize(documents: Iterable[str],\n",
    "               document_ids: Iterable,\n",
    "               offsets: Iterable[tuple[int, int]],\n",
    "               filter_len: int = 3,\n",
    "               disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into sentences. Can be used with `sectionize_documents`\n",
    "    to further split documents into more manageable pieces. Takes in offsets\n",
    "    to ensure that after splitting, the sentences can be matched to the\n",
    "    location in the original documents.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param offsets: Iterable tuple of the start and end indices\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    document_sentences = []\n",
    "    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents),\n",
    "                                              disable=disable_progress_bar):\n",
    "        try:\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            for o in sentence_offsets:\n",
    "                if o[1] - o[0] > filter_len:\n",
    "                    sentence = document[o[0]:o[1]]\n",
    "                    abs_offsets = (o[0] + offset[0], o[1] + offset[0])\n",
    "                    row = {}\n",
    "                    row['document_id'] = document_id\n",
    "                    row['text'] = sentence\n",
    "                    row['offset'] = abs_offsets\n",
    "                    document_sentences.append(row)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(document_sentences)\n",
    "\n",
    "\n",
    "def get_contexts():\n",
    "    SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n",
    "    DEVICE = 0\n",
    "    MAX_LENGTH = 384\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "    WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n",
    "    wiki_files = os.listdir(WIKI_PATH)\n",
    "\n",
    "    trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\n",
    "\n",
    "    model = SentenceTransformer(SIM_MODEL, device='cuda')\n",
    "    model.max_seq_length = MAX_LENGTH\n",
    "    model = model.half()\n",
    "\n",
    "    sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n",
    "\n",
    "    # prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    prompt_embeddings = model.encode(\n",
    "        trn.apply(lambda row: f\"{row['prompt']}\\n{row['A']}\\n{row['B']}\\n{row['C']}\\n{row['D']}\\n{row['E']}\",\n",
    "                  axis=1).values,\n",
    "        batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "    prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "    _ = gc.collect()\n",
    "\n",
    "    # Get the top 20 pages that are likely to contain the topic of interest\n",
    "    search_score, search_index = sentence_index.search(prompt_embeddings, 20)\n",
    "\n",
    "    # Save memory - delete sentence_index since it is no longer necessary\n",
    "    del sentence_index\n",
    "    del prompt_embeddings\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "\n",
    "    df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n",
    "                         columns=['id', 'file'])\n",
    "\n",
    "    # Get the article and associated file location using the index\n",
    "    wikipedia_file_data = []\n",
    "\n",
    "    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n",
    "        scr_idx = idx\n",
    "        _df = df.loc[scr_idx].copy()\n",
    "        _df['prompt_id'] = i\n",
    "        wikipedia_file_data.append(_df)\n",
    "    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "    wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(\n",
    "        ['file', 'id']).reset_index(drop=True)\n",
    "\n",
    "    # Save memory - delete df since it is no longer necessary\n",
    "    del df\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "\n",
    "    # Get the full text data\n",
    "    wiki_text_data = []\n",
    "\n",
    "    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n",
    "        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file'] == file]['id'].tolist()]\n",
    "        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text', 'title'])\n",
    "\n",
    "        _df_temp = _df[_df['id'].isin(_id)].copy()\n",
    "        del _df\n",
    "        _ = gc.collect()\n",
    "        libc.malloc_trim(0)\n",
    "        wiki_text_data.append(_df_temp)\n",
    "    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "    _ = gc.collect()\n",
    "\n",
    "    # Parse documents into sentences\n",
    "    processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n",
    "\n",
    "    # Get embeddings of the wiki text data\n",
    "    wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        device=DEVICE,\n",
    "                                        show_progress_bar=True,\n",
    "                                        convert_to_tensor=True,\n",
    "                                        normalize_embeddings=True)  # .half()\n",
    "    wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    _ = gc.collect()\n",
    "\n",
    "    # Combine all answers\n",
    "    trn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n",
    "\n",
    "    # Search using the prompt and answers to guide the search\n",
    "    trn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']\n",
    "\n",
    "    question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                                       show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    question_embeddings = question_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    # Parameter to determine how many relevant sentences to include\n",
    "    NUM_SENTENCES_INCLUDE = 6\n",
    "\n",
    "    # List containing just Context\n",
    "    contexts = []\n",
    "\n",
    "    for r in tqdm(trn.itertuples(), total=len(trn)):\n",
    "\n",
    "        prompt_id = r.Index\n",
    "\n",
    "        prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(\n",
    "            wikipedia_file_data[wikipedia_file_data['prompt_id'] == prompt_id]['id'].values)].index.values\n",
    "\n",
    "        if prompt_indices.shape[0] > 0:\n",
    "            prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "            prompt_index.add(wiki_data_embeddings[prompt_indices])\n",
    "\n",
    "            context = \"\"\n",
    "\n",
    "            # Get the top matches\n",
    "            ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n",
    "            for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n",
    "                context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n",
    "        contexts.append(context)\n",
    "\n",
    "    trn['context'] = contexts\n",
    "\n",
    "    trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "\n",
    "def generate_openbook_output():\n",
    "    test_df = pd.read_csv(\"test_context.csv\")\n",
    "    test_df.index = list(range(len(test_df)))\n",
    "    test_df['id'] = list(range(len(test_df)))\n",
    "    test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" + test_df[\"prompt\"]\n",
    "    test_df['answer'] = 'A'\n",
    "    model_dir = \"/kaggle/input/llm-science-run-context-2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "    model.eval()\n",
    "\n",
    "    # We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\n",
    "    options = 'ABCDE'\n",
    "    indices = list(range(5))\n",
    "\n",
    "    option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "    index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "\n",
    "    def preprocess(example):\n",
    "        # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n",
    "        # so we'll copy our question 5 times before tokenizing\n",
    "        first_sentence = [example['prompt']] * 5\n",
    "        second_sentence = []\n",
    "        for option in options:\n",
    "            second_sentence.append(example[option])\n",
    "        # Our tokenizer will turn our text into token IDs BERT can understand\n",
    "        tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n",
    "        tokenized_example['label'] = option_to_index[example['answer']]\n",
    "        return tokenized_example\n",
    "\n",
    "    tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "    tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "    test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "    test_predictions = []\n",
    "    i = 0\n",
    "    for batch in test_dataloader:\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        losses = -outputs.logits.cpu().detach().numpy()\n",
    "        preds = torch.softmax(torch.tensor(-losses), dim=-1)\n",
    "        if i < 10:\n",
    "            print(losses, preds)\n",
    "            i += 1\n",
    "        test_predictions.append(torch.squeeze(preds).tolist())\n",
    "    \n",
    "\n",
    "    prob_lables = ['A_prob', 'B_prob', 'C_prob', 'D_prob', 'E_prob']\n",
    "    print(np.array(test_predictions).shape)\n",
    "    df_prob = pd.DataFrame(np.array(test_predictions), index=test_df.index, columns=prob_lables)\n",
    "    df_prob.to_csv('preds_backup.csv', index=False)\n",
    "\n",
    "#     predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "\n",
    "#     predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "#     # predictions_as_answer_letters[:3]\n",
    "\n",
    "#     predictions_as_string = test_df['prediction'] = [\n",
    "#         ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "#     ]\n",
    "\n",
    "#     submission = test_df[['id', 'prediction']]\n",
    "#     submission.to_csv('submission_backup.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456c9b73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:20.549859Z",
     "iopub.status.busy": "2023-10-09T13:46:20.549534Z",
     "iopub.status.idle": "2023-10-09T13:46:20.553604Z",
     "shell.execute_reply": "2023-10-09T13:46:20.552692Z"
    },
    "papermill": {
     "duration": 0.018996,
     "end_time": "2023-10-09T13:46:20.555334",
     "exception": false,
     "start_time": "2023-10-09T13:46:20.536338",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# get_contexts()\n",
    "# #generate_openbook_output()\n",
    "\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b032c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:20.580926Z",
     "iopub.status.busy": "2023-10-09T13:46:20.580081Z",
     "iopub.status.idle": "2023-10-09T13:46:20.584703Z",
     "shell.execute_reply": "2023-10-09T13:46:20.583787Z"
    },
    "papermill": {
     "duration": 0.018989,
     "end_time": "2023-10-09T13:46:20.586432",
     "exception": false,
     "start_time": "2023-10-09T13:46:20.567443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#backup_model_predictions = pd.read_csv(\"preds_backup.csv\")\n",
    "#backup_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ccea2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:20.611891Z",
     "iopub.status.busy": "2023-10-09T13:46:20.611029Z",
     "iopub.status.idle": "2023-10-09T13:46:20.627118Z",
     "shell.execute_reply": "2023-10-09T13:46:20.626294Z"
    },
    "papermill": {
     "duration": 0.030873,
     "end_time": "2023-10-09T13:46:20.629012",
     "exception": false,
     "start_time": "2023-10-09T13:46:20.598139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForMultipleChoice\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e621672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:20.654860Z",
     "iopub.status.busy": "2023-10-09T13:46:20.654526Z",
     "iopub.status.idle": "2023-10-09T13:46:54.966839Z",
     "shell.execute_reply": "2023-10-09T13:46:54.965558Z"
    },
    "papermill": {
     "duration": 34.328039,
     "end_time": "2023-10-09T13:46:54.969156",
     "exception": false,
     "start_time": "2023-10-09T13:46:20.641117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n",
    "!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58304bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:54.995457Z",
     "iopub.status.busy": "2023-10-09T13:46:54.994904Z",
     "iopub.status.idle": "2023-10-09T13:46:55.013736Z",
     "shell.execute_reply": "2023-10-09T13:46:55.012670Z"
    },
    "papermill": {
     "duration": 0.034161,
     "end_time": "2023-10-09T13:46:55.015513",
     "exception": false,
     "start_time": "2023-10-09T13:46:54.981352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SplitList(mylist, chunk_size):\n",
    "    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n",
    "\n",
    "def get_relevant_documents_parsed(df_valid):\n",
    "    df_chunk_size=600\n",
    "    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n",
    "    modified_texts = paraphs_parsed_dataset.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"title\"],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"text\"],\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles_val = SplitList(articles_flatten, top_per_query)\n",
    "    \n",
    "    return retrieved_articles_val\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_documents(df_valid):\n",
    "    df_chunk_size=800\n",
    "    \n",
    "    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n",
    "    modified_texts = cohere_dataset_filtered.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         cohere_dataset_filtered[idx.item()][\"title\"],\n",
    "                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles_val = SplitList(articles_flatten, top_per_query)\n",
    "    \n",
    "   \n",
    "    return retrieved_articles_val\n",
    "\n",
    "\n",
    "\n",
    "def retrieval(df_valid, modified_texts):\n",
    "    options = [x for x in df_valid.columns if x in {'A', 'B', 'C', 'D', 'E'}]\n",
    "    corpus_df_valid = df_valid.apply(lambda row:\n",
    "                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n' + '\\n'.join([row[ops] for ops in options]),\n",
    "                                     axis=1).values\n",
    "    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words)\n",
    "    vectorizer1.fit(corpus_df_valid)\n",
    "    vocab_df_valid = vectorizer1.get_feature_names_out()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words,\n",
    "                                 vocabulary=vocab_df_valid)\n",
    "    vectorizer.fit(modified_texts[:500000])\n",
    "    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n",
    "    \n",
    "    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    chunk_size = 100000\n",
    "    top_per_chunk = 10\n",
    "    top_per_query = 10\n",
    "\n",
    "    all_chunk_top_indices = []\n",
    "    all_chunk_top_values = []\n",
    "\n",
    "    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n",
    "        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n",
    "        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n",
    "        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n",
    "        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n",
    "\n",
    "        all_chunk_top_indices.append(chunk_top_indices + idx)\n",
    "        all_chunk_top_values.append(chunk_top_values)\n",
    "\n",
    "    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n",
    "    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n",
    "    \n",
    "    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n",
    "    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n",
    "    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n",
    "    \n",
    "    return articles_indices, merged_top_scores\n",
    "\n",
    "\n",
    "def prepare_answering_input(\n",
    "        tokenizer, \n",
    "        question,  \n",
    "        options,   \n",
    "        context,   \n",
    "        max_seq_length=4096,\n",
    "    ):\n",
    "    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n",
    "    c_plus_q_4 = [c_plus_q] * len(options)\n",
    "    tokenized_examples = tokenizer(\n",
    "        c_plus_q_4, options,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"longest\",\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n",
    "    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n",
    "    example_encoded = {\n",
    "        \"input_ids\": input_ids.to(model.device.index),\n",
    "        \"attention_mask\": attention_mask.to(model.device.index),\n",
    "    }\n",
    "    return example_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c512851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.041367Z",
     "iopub.status.busy": "2023-10-09T13:46:55.040660Z",
     "iopub.status.idle": "2023-10-09T13:46:55.047933Z",
     "shell.execute_reply": "2023-10-09T13:46:55.047126Z"
    },
    "papermill": {
     "duration": 0.022037,
     "end_time": "2023-10-09T13:46:55.049660",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.027623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = ['each', 'you', 'the', 'use', 'used',\n",
    "                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n",
    "                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n",
    "                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n",
    "                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n",
    "                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n",
    "                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n",
    "                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n",
    "                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n",
    "                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n",
    "                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n",
    "                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n",
    "                  'did', 'theirs', 'can', 'those',\n",
    "                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n",
    "                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n",
    "                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n",
    "                  'yours', 'but', 'being', \"wasn't\", 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69f13f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.075454Z",
     "iopub.status.busy": "2023-10-09T13:46:55.075099Z",
     "iopub.status.idle": "2023-10-09T13:46:55.089093Z",
     "shell.execute_reply": "2023-10-09T13:46:55.088184Z"
    },
    "papermill": {
     "duration": 0.029195,
     "end_time": "2023-10-09T13:46:55.091231",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.062036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')\n",
    "test_df['answer'] = 'A' # dummy answer that allows us to preprocess the test datataset using functionality that works for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f3dfa4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.117910Z",
     "iopub.status.busy": "2023-10-09T13:46:55.117581Z",
     "iopub.status.idle": "2023-10-09T13:46:55.122214Z",
     "shell.execute_reply": "2023-10-09T13:46:55.121269Z"
    },
    "papermill": {
     "duration": 0.019586,
     "end_time": "2023-10-09T13:46:55.123937",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.104351",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    test_retrieved_articles_parsed = get_relevant_documents_parsed(test_df)\n",
    "    gc.collect()\n",
    "\n",
    "    test_retrieved_articles = get_relevant_documents(test_df)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "559dc838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.151496Z",
     "iopub.status.busy": "2023-10-09T13:46:55.150963Z",
     "iopub.status.idle": "2023-10-09T13:46:55.155422Z",
     "shell.execute_reply": "2023-10-09T13:46:55.154506Z"
    },
    "papermill": {
     "duration": 0.019044,
     "end_time": "2023-10-09T13:46:55.157104",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.138060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\n",
    "    model = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b592270d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.182858Z",
     "iopub.status.busy": "2023-10-09T13:46:55.181988Z",
     "iopub.status.idle": "2023-10-09T13:46:55.190471Z",
     "shell.execute_reply": "2023-10-09T13:46:55.189639Z"
    },
    "papermill": {
     "duration": 0.023324,
     "end_time": "2023-10-09T13:46:55.192267",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.168943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def longformer_predict(df, retrieved_articles, retrieved_articles_parsed):\n",
    "    probabilities = []\n",
    "    for index in tqdm(range(df.shape[0])):\n",
    "        columns = df.iloc[index].values\n",
    "        question = columns[1]\n",
    "        options = [columns[2], columns[3], columns[4], columns[5], columns[6]]   #[columns[2], columns[3], columns[4], columns[5], columns[6]]\n",
    "        context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n",
    "        context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n",
    "        inputs1 = prepare_answering_input(\n",
    "            tokenizer=tokenizer, question=question,\n",
    "            options=options, context=context1,\n",
    "            )\n",
    "        inputs2 = prepare_answering_input(\n",
    "            tokenizer=tokenizer, question=question,\n",
    "            options=options, context=context2,\n",
    "            )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model(**inputs1)    \n",
    "            losses1 = -outputs1.logits[0].detach().cpu().numpy()\n",
    "            probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs2 = model(**inputs2)\n",
    "            losses2 = -outputs2.logits[0].detach().cpu().numpy()\n",
    "            probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n",
    "\n",
    "        probability_ = (probability1 + probability2)/2\n",
    "     #   if probability_.max() < 0.4:\n",
    "      #      probability_ = list(backup_model_predictions.iloc[index, :5])\n",
    "      #      probabilities.append(probability_)\n",
    "      #  else:\n",
    "        probabilities.append(probability_.tolist())\n",
    "    return np.array(probabilities)   #torch.cat(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74e987ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.218761Z",
     "iopub.status.busy": "2023-10-09T13:46:55.217787Z",
     "iopub.status.idle": "2023-10-09T13:46:55.222633Z",
     "shell.execute_reply": "2023-10-09T13:46:55.221796Z"
    },
    "papermill": {
     "duration": 0.019637,
     "end_time": "2023-10-09T13:46:55.224405",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.204768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    longtrans_preds = longformer_predict(test_df, test_retrieved_articles, test_retrieved_articles_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b09ee3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.249720Z",
     "iopub.status.busy": "2023-10-09T13:46:55.248887Z",
     "iopub.status.idle": "2023-10-09T13:46:55.253473Z",
     "shell.execute_reply": "2023-10-09T13:46:55.252672Z"
    },
    "papermill": {
     "duration": 0.019167,
     "end_time": "2023-10-09T13:46:55.255137",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.235970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    del test_retrieved_articles_parsed, test_retrieved_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032431b9",
   "metadata": {
    "papermill": {
     "duration": 0.011503,
     "end_time": "2023-10-09T13:46:55.278116",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.266613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Getting Sentences from the Relevant Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a93fe42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.303156Z",
     "iopub.status.busy": "2023-10-09T13:46:55.302842Z",
     "iopub.status.idle": "2023-10-09T13:46:55.312728Z",
     "shell.execute_reply": "2023-10-09T13:46:55.311692Z"
    },
    "papermill": {
     "duration": 0.024673,
     "end_time": "2023-10-09T13:46:55.314519",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.289846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    model = SentenceTransformer(SIM_MODEL, device='cuda')\n",
    "    model.max_seq_length = MAX_LENGTH\n",
    "    model = model.half()\n",
    "\n",
    "    df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\", columns=['id', 'file'])\n",
    "\n",
    "    ## Get the article and associated file location using the index\n",
    "    wikipedia_file_data = []\n",
    "\n",
    "    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n",
    "        scr_idx = idx\n",
    "        _df = df.loc[scr_idx].copy()\n",
    "        _df['prompt_id'] = i\n",
    "        wikipedia_file_data.append(_df)\n",
    "    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "    wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    del df\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "\n",
    "    ## Get the full text data\n",
    "    wiki_text_data = []\n",
    "\n",
    "    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n",
    "        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n",
    "        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n",
    "\n",
    "        _df_temp = _df[_df['id'].isin(_id)].copy()\n",
    "        del _df\n",
    "        _ = gc.collect()\n",
    "        libc.malloc_trim(0)\n",
    "        wiki_text_data.append(_df_temp)\n",
    "    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    _ = gc.collect()\n",
    "\n",
    "    processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n",
    "\n",
    "\n",
    "    wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        device=DEVICE,\n",
    "                                        show_progress_bar=True,\n",
    "                                        convert_to_tensor=True,\n",
    "                                        normalize_embeddings=True)#.half()\n",
    "    wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    trn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n",
    "    trn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']\n",
    "\n",
    "    question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    question_embeddings = question_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfe202",
   "metadata": {
    "papermill": {
     "duration": 0.011239,
     "end_time": "2023-10-09T13:46:55.337399",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.326160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extracting Matching Prompt-Sentence Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50a0044e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.362063Z",
     "iopub.status.busy": "2023-10-09T13:46:55.361430Z",
     "iopub.status.idle": "2023-10-09T13:46:55.367967Z",
     "shell.execute_reply": "2023-10-09T13:46:55.366971Z"
    },
    "papermill": {
     "duration": 0.020908,
     "end_time": "2023-10-09T13:46:55.369732",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.348824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    contexts = []\n",
    "\n",
    "    for r in tqdm(trn.itertuples(), total=len(trn)):\n",
    "\n",
    "        prompt_id = r.Index\n",
    "\n",
    "        prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n",
    "\n",
    "        if prompt_indices.shape[0] > 0:\n",
    "            prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "            prompt_index.add(wiki_data_embeddings[prompt_indices])\n",
    "\n",
    "            context = \"\"\n",
    "\n",
    "            ## Get the top matches\n",
    "            ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n",
    "            for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n",
    "                context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n",
    "\n",
    "        contexts.append(context)\n",
    "    trn['context'] = contexts\n",
    "    trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69593f0b",
   "metadata": {
    "papermill": {
     "duration": 0.0113,
     "end_time": "2023-10-09T13:46:55.392253",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.380953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc967eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.416575Z",
     "iopub.status.busy": "2023-10-09T13:46:55.416046Z",
     "iopub.status.idle": "2023-10-09T13:46:55.421210Z",
     "shell.execute_reply": "2023-10-09T13:46:55.420295Z"
    },
    "papermill": {
     "duration": 0.01937,
     "end_time": "2023-10-09T13:46:55.422961",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.403591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    test_df = pd.read_csv(\"test_context.csv\")\n",
    "    test_df.index = list(range(len(test_df)))\n",
    "    test_df['id'] = list(range(len(test_df)))\n",
    "    test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:CONTEXT_LEN]) + \" #### \" +  test_df[\"prompt\"]\n",
    "    test_df['answer'] = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5aa8524d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.447767Z",
     "iopub.status.busy": "2023-10-09T13:46:55.446856Z",
     "iopub.status.idle": "2023-10-09T13:46:55.451807Z",
     "shell.execute_reply": "2023-10-09T13:46:55.450972Z"
    },
    "papermill": {
     "duration": 0.019093,
     "end_time": "2023-10-09T13:46:55.453501",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.434408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    model_dir = \"/kaggle/input/llm-science-run-context-2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4537dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.479002Z",
     "iopub.status.busy": "2023-10-09T13:46:55.478033Z",
     "iopub.status.idle": "2023-10-09T13:46:55.484361Z",
     "shell.execute_reply": "2023-10-09T13:46:55.483534Z"
    },
    "papermill": {
     "duration": 0.02116,
     "end_time": "2023-10-09T13:46:55.486088",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.464928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "options = 'ABCDE'\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "\n",
    "def preprocess(example):\n",
    "  \n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentence = []\n",
    "    for option in options:\n",
    "        second_sentence.append(example[option])\n",
    "    \n",
    "    tokenized_example = tokenizer(first_sentence, second_sentence, truncation='only_first')\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    return tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d9a30b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.511574Z",
     "iopub.status.busy": "2023-10-09T13:46:55.511031Z",
     "iopub.status.idle": "2023-10-09T13:46:55.518894Z",
     "shell.execute_reply": "2023-10-09T13:46:55.517908Z"
    },
    "papermill": {
     "duration": 0.022534,
     "end_time": "2023-10-09T13:46:55.520631",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.498097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09e27b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.546072Z",
     "iopub.status.busy": "2023-10-09T13:46:55.545416Z",
     "iopub.status.idle": "2023-10-09T13:46:55.550467Z",
     "shell.execute_reply": "2023-10-09T13:46:55.549535Z"
    },
    "papermill": {
     "duration": 0.01978,
     "end_time": "2023-10-09T13:46:55.552112",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.532332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "    tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "    test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ce9ec6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.576872Z",
     "iopub.status.busy": "2023-10-09T13:46:55.576071Z",
     "iopub.status.idle": "2023-10-09T13:46:55.581902Z",
     "shell.execute_reply": "2023-10-09T13:46:55.581122Z"
    },
    "papermill": {
     "duration": 0.019857,
     "end_time": "2023-10-09T13:46:55.583588",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.563731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    test_predictions = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        test_predictions.append(outputs.logits.cpu().detach())\n",
    "\n",
    "    test_predictions = torch.cat(test_predictions)\n",
    "    test_predictions = softmax(test_predictions, axis=1).numpy()\n",
    "    ob_preds = test_predictions\n",
    "    del test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5448eb26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.609745Z",
     "iopub.status.busy": "2023-10-09T13:46:55.608745Z",
     "iopub.status.idle": "2023-10-09T13:46:55.615384Z",
     "shell.execute_reply": "2023-10-09T13:46:55.614563Z"
    },
    "papermill": {
     "duration": 0.021475,
     "end_time": "2023-10-09T13:46:55.617187",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.595712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    # model_dir = \"/kaggle/input/how-to-train-open-book-model-part-1/model_v2\"\n",
    "    model_dir = \"/kaggle/input/checkpoint-5975-09025\"\n",
    "    # model_dir = \"/kaggle/input/checkpoint-7100-09108\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    test_predictionsc = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        test_predictionsc.append(outputs.logits.cpu().detach())\n",
    "\n",
    "    test_predictionsc = torch.cat(test_predictionsc)\n",
    "    test_predictionsc = softmax(test_predictionsc, axis=1).numpy()    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be709bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.642632Z",
     "iopub.status.busy": "2023-10-09T13:46:55.641792Z",
     "iopub.status.idle": "2023-10-09T13:46:55.648067Z",
     "shell.execute_reply": "2023-10-09T13:46:55.647218Z"
    },
    "papermill": {
     "duration": 0.021027,
     "end_time": "2023-10-09T13:46:55.649807",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.628780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    model_dir = \"/kaggle/input/using-deepspeed-with-hf-trainer/checkpoints_1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "    model.eval()\n",
    "\n",
    "    test_predictionsi = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        test_predictionsi.append(outputs.logits.cpu().detach())\n",
    "\n",
    "    test_predictionsi = torch.cat(test_predictionsi)\n",
    "    test_predictionsi = softmax(test_predictionsi, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cad5f",
   "metadata": {
    "papermill": {
     "duration": 0.011673,
     "end_time": "2023-10-09T13:46:55.673581",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.661908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### In order to increase diversity, we also use some weights that do not use openbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2c3390b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.698607Z",
     "iopub.status.busy": "2023-10-09T13:46:55.697639Z",
     "iopub.status.idle": "2023-10-09T13:46:55.703695Z",
     "shell.execute_reply": "2023-10-09T13:46:55.702877Z"
    },
    "papermill": {
     "duration": 0.020391,
     "end_time": "2023-10-09T13:46:55.705389",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.684998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "deberta_v3_large = '/kaggle/input/deberta-v3-large-hf-weights'\n",
    "import os\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b483937",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.730760Z",
     "iopub.status.busy": "2023-10-09T13:46:55.729821Z",
     "iopub.status.idle": "2023-10-09T13:46:55.739097Z",
     "shell.execute_reply": "2023-10-09T13:46:55.738284Z"
    },
    "papermill": {
     "duration": 0.023528,
     "end_time": "2023-10-09T13:46:55.740820",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.717292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentences = [example[option] for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=False)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "457f4523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.765945Z",
     "iopub.status.busy": "2023-10-09T13:46:55.765426Z",
     "iopub.status.idle": "2023-10-09T13:46:55.771106Z",
     "shell.execute_reply": "2023-10-09T13:46:55.770068Z"
    },
    "papermill": {
     "duration": 0.02011,
     "end_time": "2023-10-09T13:46:55.772897",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.752787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)\n",
    "\n",
    "    test_df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')\n",
    "    test_df['answer'] = 'A' # dummy answer that allows us to preprocess the test datataset using functionality that works for the train set\n",
    "\n",
    "\n",
    "    tokenized_test_dataset = Dataset.from_pandas(test_df.drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "    test_dataloader = DataLoader(tokenized_test_dataset, 1, shuffle=False, collate_fn=data_collator, num_workers=0, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "422323ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.797850Z",
     "iopub.status.busy": "2023-10-09T13:46:55.797281Z",
     "iopub.status.idle": "2023-10-09T13:46:55.803030Z",
     "shell.execute_reply": "2023-10-09T13:46:55.802011Z"
    },
    "papermill": {
     "duration": 0.020311,
     "end_time": "2023-10-09T13:46:55.804760",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.784449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(f'/kaggle/input/2023kagglellm-deberta-v3-large-model1').cuda()\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        preds.append(outputs.logits.cpu().detach())\n",
    "\n",
    "    hyc_preds = torch.cat(preds)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    hyc_preds = softmax(hyc_preds, axis=1).numpy()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414eee81",
   "metadata": {
    "papermill": {
     "duration": 0.011496,
     "end_time": "2023-10-09T13:46:55.827668",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.816172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71962382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:55.852890Z",
     "iopub.status.busy": "2023-10-09T13:46:55.852001Z",
     "iopub.status.idle": "2023-10-09T13:46:56.387623Z",
     "shell.execute_reply": "2023-10-09T13:46:56.386685Z"
    },
    "papermill": {
     "duration": 0.550229,
     "end_time": "2023-10-09T13:46:56.389569",
     "exception": false,
     "start_time": "2023-10-09T13:46:55.839340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efc40208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:57.265553Z",
     "iopub.status.busy": "2023-10-09T13:46:57.265082Z",
     "iopub.status.idle": "2023-10-09T13:46:57.279073Z",
     "shell.execute_reply": "2023-10-09T13:46:57.277874Z"
    },
    "papermill": {
     "duration": 0.099389,
     "end_time": "2023-10-09T13:46:57.282456",
     "exception": false,
     "start_time": "2023-10-09T13:46:57.183067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09ff4e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:57.432596Z",
     "iopub.status.busy": "2023-10-09T13:46:57.430252Z",
     "iopub.status.idle": "2023-10-09T13:46:57.438684Z",
     "shell.execute_reply": "2023-10-09T13:46:57.437609Z"
    },
    "papermill": {
     "duration": 0.096541,
     "end_time": "2023-10-09T13:46:57.442460",
     "exception": false,
     "start_time": "2023-10-09T13:46:57.345919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = '/kaggle/input/llm-kaggle-awp'\n",
    "CONF_PATH = MODEL_DIR + '/deberta-v3-large_config.pth'\n",
    "MODEL_PATH = MODEL_DIR + '/best_model_public.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "569c80ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:57.539879Z",
     "iopub.status.busy": "2023-10-09T13:46:57.539486Z",
     "iopub.status.idle": "2023-10-09T13:46:57.618125Z",
     "shell.execute_reply": "2023-10-09T13:46:57.617512Z"
    },
    "papermill": {
     "duration": 0.112978,
     "end_time": "2023-10-09T13:46:57.619815",
     "exception": false,
     "start_time": "2023-10-09T13:46:57.506837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca183562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:46:57.645321Z",
     "iopub.status.busy": "2023-10-09T13:46:57.644972Z",
     "iopub.status.idle": "2023-10-09T13:47:01.314180Z",
     "shell.execute_reply": "2023-10-09T13:47:01.313157Z"
    },
    "papermill": {
     "duration": 3.684831,
     "end_time": "2023-10-09T13:47:01.316724",
     "exception": false,
     "start_time": "2023-10-09T13:46:57.631893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')\n",
    "test_df['answer'] = 'A' # dummy answer that allows us to preprocess the test datataset using functionality that works for the train set\n",
    "test_df = test_df.replace(np.NaN, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "018c1eba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:47:01.347162Z",
     "iopub.status.busy": "2023-10-09T13:47:01.346719Z",
     "iopub.status.idle": "2023-10-09T13:47:01.351580Z",
     "shell.execute_reply": "2023-10-09T13:47:01.350603Z"
    },
    "papermill": {
     "duration": 0.02307,
     "end_time": "2023-10-09T13:47:01.354228",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.331158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR+'/tokenizer')\n",
    "    tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c845a15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:47:01.384596Z",
     "iopub.status.busy": "2023-10-09T13:47:01.384228Z",
     "iopub.status.idle": "2023-10-09T13:47:01.400616Z",
     "shell.execute_reply": "2023-10-09T13:47:01.399733Z"
    },
    "papermill": {
     "duration": 0.034677,
     "end_time": "2023-10-09T13:47:01.403180",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.368503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlmseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.a2i = {alp: idx for idx, alp in enumerate('ABCDE')}\n",
    "        self.i2a = {v: k for k,v in self.a2i.items()}\n",
    "        self.perm_dict = {0: [1,2,3,4],\n",
    "                     1: [2,3,4,0], \n",
    "                     2: [3,4,0,1],\n",
    "                     3: [4,0,1,2],\n",
    "                     4: [0,1,2,3]}\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.df.iloc[idx]\n",
    "        tokenized_example = dict()              \n",
    "\n",
    "        first_sentence = [example['prompt']] * 5\n",
    "        second_sentences = [example[option] for option in 'ABCDE']\n",
    "        other_sentences = [[] for i in range(5)]\n",
    "\n",
    "        for i, p in enumerate(range(5)):\n",
    "            value = self.perm_dict[p] \n",
    "            for v in value:\n",
    "                al = self.i2a[v] \n",
    "                second_sentences[i]+= ' ' + example[al]\n",
    "\n",
    "        tokenized_example = tokenizer(first_sentence, \n",
    "                                      second_sentences,\n",
    "                                      truncation='only_first')\n",
    "        tokenized_example['label'] = option_to_index[example['answer']]\n",
    "        return tokenized_example\n",
    "            \n",
    "test_ds = LlmseDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba82329e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:47:01.469007Z",
     "iopub.status.busy": "2023-10-09T13:47:01.468524Z",
     "iopub.status.idle": "2023-10-09T13:47:01.477957Z",
     "shell.execute_reply": "2023-10-09T13:47:01.476612Z"
    },
    "papermill": {
     "duration": 0.047861,
     "end_time": "2023-10-09T13:47:01.483839",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.435978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    test_dl = DataLoader(\n",
    "        test_ds, \n",
    "        batch_size=1, \n",
    "        shuffle=False, \n",
    "        collate_fn=data_collator,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c80a3d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:47:01.511832Z",
     "iopub.status.busy": "2023-10-09T13:47:01.511491Z",
     "iopub.status.idle": "2023-10-09T13:47:01.517601Z",
     "shell.execute_reply": "2023-10-09T13:47:01.516609Z"
    },
    "papermill": {
     "duration": 0.022416,
     "end_time": "2023-10-09T13:47:01.519269",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.496853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_conf, *, dropout=0.2, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Transformer\n",
    "        #self.config = AutoConfig.from_pretrained(model_conf)\n",
    "\n",
    "        self.transformer = AutoModelForMultipleChoice.from_config(model_conf)\n",
    "\n",
    "        #self._init_weights(self.fc, self.config)\n",
    "\n",
    "    def _init_weights(self, module, config):\n",
    "        module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        out = self.transformer(input_ids, attention_mask, token_type_ids=token_type_ids)\n",
    "        x = out['logits'] \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eaeb9cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:47:01.549028Z",
     "iopub.status.busy": "2023-10-09T13:47:01.548698Z",
     "iopub.status.idle": "2023-10-09T13:47:01.555619Z",
     "shell.execute_reply": "2023-10-09T13:47:01.554612Z"
    },
    "papermill": {
     "duration": 0.024062,
     "end_time": "2023-10-09T13:47:01.557285",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.533223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_TEST_SET:\n",
    "    config = torch.load(CONF_PATH)\n",
    "    model = CustomModel(model_conf=config)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    y_preds = []\n",
    "\n",
    "    with tqdm(test_dl, leave=True) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(pbar):\n",
    "                inp_ids = batch['input_ids'].to(device)\n",
    "                att_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "                y_pred = model(input_ids=inp_ids, \n",
    "                               attention_mask=att_mask, \n",
    "                               token_type_ids=token_type_ids)\n",
    "\n",
    "                y_pred = y_pred.to(torch.float)\n",
    "\n",
    "                y_preds.append(y_pred.cpu())\n",
    "\n",
    "\n",
    "    itk_preds = torch.cat(y_preds)\n",
    "    del model, y_preds\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    itk_preds = softmax(itk_preds, axis=1).numpy()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd704a41",
   "metadata": {
    "papermill": {
     "duration": 0.01203,
     "end_time": "2023-10-09T13:47:01.581955",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.569925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Blending Weights Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a8681",
   "metadata": {
    "papermill": {
     "duration": 0.013248,
     "end_time": "2023-10-09T13:47:01.607386",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.594138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Maximising MAP@3 is very difficult(Is it even possible?). so Minimising CE loss here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1c11b",
   "metadata": {
    "papermill": {
     "duration": 0.012329,
     "end_time": "2023-10-09T13:47:01.631477",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.619148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Apply weights and make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3f2a075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T13:47:01.661519Z",
     "iopub.status.busy": "2023-10-09T13:47:01.659549Z",
     "iopub.status.idle": "2023-10-09T13:47:01.680417Z",
     "shell.execute_reply": "2023-10-09T13:47:01.678773Z"
    },
    "papermill": {
     "duration": 0.039021,
     "end_time": "2023-10-09T13:47:01.682195",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.643174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>A B C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id prediction\n",
       "0   0      A B C\n",
       "1   1      A B C\n",
       "2   2      A B C\n",
       "3   3      A B C\n",
       "4   4      A B C\n",
       "5   5      A B C\n",
       "6   6      A B C\n",
       "7   7      A B C\n",
       "8   8      A B C\n",
       "9   9      A B C"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if IS_TEST_SET:\n",
    "#     ws = [1/6., 1/6., 1/6., 1/6., 1/6., 1/6.]\n",
    "#     ws = [6.47750967e-01, 3.86703564e-18, 7.57196376e-03, 7.00792352e-02, 2.46658558e-01, 2.79392764e-02]\n",
    "#     ws = [0.411877448, 8.8371708e-20, 1.736642515e-18, 0.00924490935, 0.0788776425, 0.5]\n",
    "#     ws = [2.66547532e-01, 1.59127328e-18, 3.11583981e-03, 2.88373899e-02, 1.01499238e-01, 6.00000000e-01]\n",
    "#     ws = [6.66368830e-01, 3.97818320e-18, 7.78959953e-03, 7.20934747e-02, 2.53748096e-01]\n",
    "    ws = [0.399821298, 2.38690992e-18, 0.00467375972, 0.0432560848, 0.152248858, 0.4]\n",
    "    predictions_overall = test_predictionsc * ws[0] + ob_preds * ws[1] + test_predictionsi * ws[2] + hyc_preds * ws[3] + itk_preds * ws[4] + longtrans_preds * ws[5]\n",
    "    print(predictions_overall.shape)\n",
    "#     predictions_overall = np.where(np.logical_and(longtrans_preds.max(axis=1) < 0.4, predictions_overall.max(axis=1) > 0.4).reshape(-1,1).repeat(5, axis=1), predictions_overall, longtrans_preds)\n",
    "\n",
    "    predictions_overall = predictions_overall\n",
    "    predictions_overall = np.argsort(-predictions_overall)[:,:3]\n",
    "    print(predictions_overall[:5])\n",
    "\n",
    "    predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n",
    "    print(predictions_as_answer_letters[:3])\n",
    "\n",
    "    predictions_as_string = test_df['prediction'] = [\n",
    "        ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "    ]\n",
    "    print(predictions_as_string[:3])\n",
    "\n",
    "else:\n",
    "    test_df['prediction'] = 'A B C'\n",
    "    \n",
    "submission = test_df[['id', 'prediction']]\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "pd.read_csv('submission.csv').head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710775c9",
   "metadata": {
    "papermill": {
     "duration": 0.012077,
     "end_time": "2023-10-09T13:47:01.707572",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.695495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661a0e8",
   "metadata": {
    "papermill": {
     "duration": 0.013092,
     "end_time": "2023-10-09T13:47:01.732974",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.719882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6670c8f",
   "metadata": {
    "papermill": {
     "duration": 0.019377,
     "end_time": "2023-10-09T13:47:01.765158",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.745781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0476e3c",
   "metadata": {
    "papermill": {
     "duration": 0.011762,
     "end_time": "2023-10-09T13:47:01.788713",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.776951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b85f5",
   "metadata": {
    "papermill": {
     "duration": 0.011399,
     "end_time": "2023-10-09T13:47:01.812306",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.800907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e56de12",
   "metadata": {
    "papermill": {
     "duration": 0.012579,
     "end_time": "2023-10-09T13:47:01.836628",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.824049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In conclusion, at least we were able to confirm that the openbook model (based on Ozturk's and Chris'), which differs in method from other models and has a high score, has the higher weight.\n",
    "\n",
    "Now it's your turn to blend. Let's add weights for your model. \n",
    "\n",
    "Also, running notebooks, especially inference for openbook model, takes a long time, so it's a good idea to separate notebooks for calculating weights and for submitting them like Yirun Zhangs' base notebook.\n",
    "\n",
    "It would also be important to change the evaluation dataset to something relevant to STEM. If the model weights are unnaturally high, suspect a leak. And make sure the evaluation dataset is not used for training.\n",
    "\n",
    "### Wishing you happy kaggling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7957b6",
   "metadata": {
    "papermill": {
     "duration": 0.012131,
     "end_time": "2023-10-09T13:47:01.860955",
     "exception": false,
     "start_time": "2023-10-09T13:47:01.848824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 237.506101,
   "end_time": "2023-10-09T13:47:05.118385",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-09T13:43:07.612284",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
