{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPxTHZQA+09mXvRLSPYFhFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbtmddn41/deep_learning/blob/main/practice/text_create.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5F51-KPpAH1",
        "outputId": "c7f4387a-c95a-46ad-a6d3-2ca2f6d799f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-25 18:05:07--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  20.4MB/s    in 6.9s    \n",
            "\n",
            "2023-05-25 18:05:14 (11.7 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "KzHhUfknpPt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = keras.utils.text_dataset_from_directory(directory='aclImdb', label_mode=None, batch_size = 64)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL3zSxL6pYWM",
        "outputId": "8b710af1-b064-43f5-b99c-c20298105c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import TextVectorization\n",
        "\n",
        "sequence_len = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_len)\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "ES_G2v1Zpw8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    vectorized_sequences = text_vectorization(text_batch)\n",
        "    x = vectorized_sequences[:, :-1]\n",
        "    y = vectorized_sequences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=16)"
      ],
      "metadata": {
        "id": "IQr9f8rhqs1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, dropout_rate=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=self.dropout_rate)\n",
        "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation='relu'), layers.Dense(embed_dim)])\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(query=inputs, value=inputs, attention_mask=mask)\n",
        "        x = inputs + self.dropout1(attention_output)\n",
        "        x = self.layernorm_1(x)\n",
        "        proj_output = self.dense_proj(x)\n",
        "        x = self.dropout2(proj_output) + x\n",
        "        output = self.layernorm_2(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"embed_dim\": self.embed_dim, \"dense_dim\": self.dense_dim, \"num_heads\": self.num_heads, 'dropout_rate': self.dropout_rate})\n",
        "        return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_len, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)#, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_len, output_dim=output_dim)\n",
        "        self.sequence_len = sequence_len\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"sequence_len\": self.sequence_len, \"input_dim\": self.input_dim, \"output_dim\": self.output_dim})\n",
        "        return config\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, num_blocks, sequence_len, vocab_size, embed_dim, dense_dim, num_heads, dropout_rate=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_blocks = num_blocks\n",
        "        self.sequence_len = sequence_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(sequence_len, vocab_size, embed_dim)\n",
        "        block_layers = []\n",
        "        for i in range(num_blocks):\n",
        "            block_layers.append(TransformerEncoderBlock(embed_dim, dense_dim, num_heads, dropout_rate))\n",
        "        self.block_layers = keras.Sequential(block_layers)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pos_inputs = self.pos_embedding(inputs)\n",
        "        encoded = self.block_layers(pos_inputs)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'num_blocks': self.num_blocks, 'sequence_len': self.sequence_len,\n",
        "            'vocab_size': self.vocab_size, 'embed_dim': self.embed_dim,\n",
        "            'dense_dim': self.dense_dim, 'num_heads': self.num_heads,\n",
        "            'dropout_rate': self.dropout_rate\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, dropout_rate=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate)\n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate)\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "        self.dropout3 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"embed_dim\": self.embed_dim, \"dense_dim\": self.dense_dim, \"num_heads\": self.num_heads, 'dropout_rate': self.dropout_rate})\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_len = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_len)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_len)\n",
        "        mask = tf.cast(i >= j, dtype='int32')\n",
        "        mask = tf.reshape(mask, (1, sequence_len, sequence_len))\n",
        "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(query=inputs, key=inputs, value=inputs, attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(self.dropout1(attention_output_1)+inputs)\n",
        "        attention_output_2 = self.attention_2(query=attention_output_1, key=encoder_outputs, value=encoder_outputs, attention_mask=padding_mask)\n",
        "        attention_output_2 = self.layernorm_2(self.dropout2(attention_output_2) + attention_output_1)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        outputs = self.layernorm_3(attention_output_2 + self.dropout3(proj_output))\n",
        "        return outputs\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, num_blocks, sequence_len, vocab_size, embed_dim, dense_dim, num_heads, dropout_rate=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_blocks = num_blocks\n",
        "        self.sequence_len = sequence_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(sequence_len, vocab_size, embed_dim)\n",
        "        self.block_layers = []\n",
        "        for i in range(num_blocks):\n",
        "            self.block_layers.append(TransformerDecoderBlock(embed_dim, dense_dim, num_heads, dropout_rate))\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.dense = layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, self_encode=False):\n",
        "        x = self.pos_embedding(inputs)\n",
        "        if self_encode:\n",
        "            encoder_outputs = x\n",
        "        for decoder_block in self.block_layers:\n",
        "            x = decoder_block(x, encoder_outputs)\n",
        "        x = self.dropout(x)\n",
        "        outputs = self.dense(x)\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'num_blocks': self.num_blocks, 'sequence_len': self.sequence_len,\n",
        "            'vocab_size': self.vocab_size, 'embed_dim': self.embed_dim,\n",
        "            'dense_dim': self.dense_dim, 'num_heads': self.num_heads,\n",
        "            'dropout_rate': self.dropout_rate\n",
        "            })\n",
        "        return config"
      ],
      "metadata": {
        "id": "zHbH9HXKrT4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_blocks = 5\n",
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 34\n",
        "\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(sequence_len-1), dtype='int32', name='english')\n",
        "decoder_outputs = TransformerDecoder(num_blocks, sequence_len, vocab_size, embed_dim, dense_dim, num_heads, 0.1)(decoder_inputs, None, self_encode=True)\n",
        "transformer_decoder = keras.Model(decoder_inputs, decoder_outputs, name='transformer_decoder')\n",
        "transformer_decoder.summary()\n",
        "transformer_decoder.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE7S5UYBswE1",
        "outputId": "172a1306-cda9-4eb8-bcd9-e8542cfd7c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " english (InputLayer)        [(None, 99)]              0         \n",
            "                                                                 \n",
            " transformer_decoder (Transf  (None, 99, 15000)        102375320 \n",
            " ormerDecoder)                                                   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 102,375,320\n",
            "Trainable params: 102,375,320\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization(['hello']).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ3v_U3t8dPK",
        "outputId": "18e7ee41-17ce-4705-ca55-37be039cc5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "vocab = text_vectorization.get_vocabulary()\n",
        "tokens_index_lookup = np.pad(np.array(vocab), pad_width=(0,vocab_size), mode='constant', constant_values='[UNK]')[:vocab_size]\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, prompt, generate_length, model_input_length, temperatures=(1.,), print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch+1) % self.print_freq != 0:\n",
        "            return\n",
        "        for temperature in self.temperatures:\n",
        "            sentence = self.prompt\n",
        "            tokenized_sentence = text_vectorization([sentence])[:, :-1]\n",
        "            tokenized_sentence = tf.Variable(initial_value=tokenized_sentence)\n",
        "            for i in tqdm(range(self.generate_length), desc=f\"===={temperature}의 온도로 생성 중\"):\n",
        "                predictions = self.model.predict(tokenized_sentence, verbose=False)\n",
        "                next_token = sample_next(predictions[0, i, :])\n",
        "                sampled_token = tokens_index_lookup[next_token]\n",
        "                sentence += \" \" + sampled_token\n",
        "                if i < sequence_len-1:\n",
        "                    tokenized_sentence[0, i+1, tf.newaxis].assign(next_token)\n",
        "            print(sentence)"
      ],
      "metadata": {
        "id": "WRldkJ73tsEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(prompt, generate_length=50, model_input_length=sequence_len, temperatures=(0.2, 0,5, 0.7, 1., 1.5), print_freq=25)"
      ],
      "metadata": {
        "id": "K9YgmnNj8uaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in lm_dataset.take(1):\n",
        "    print(tf.shape(x), tf.shape(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udnDTOMa-0m6",
        "outputId": "46f3b2ed-5266-42a3-d8e5-a8baa04b2d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([256  99], shape=(2,), dtype=int32) tf.Tensor([256  99], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_decoder.fit(lm_dataset.prefetch(8), epochs=200, callbacks=[text_gen_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzlMhNJj9bzy",
        "outputId": "3622106b-4d90-4af7-e25c-917d0180733a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1563/1563 [==============================] - 860s 541ms/step - loss: 5.6346\n",
            "Epoch 2/200\n",
            "1563/1563 [==============================] - 845s 540ms/step - loss: 5.0039\n",
            "Epoch 3/200\n",
            " 381/1563 [======>.......................] - ETA: 10:38 - loss: 4.8838"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P2_OeQz3BllL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}